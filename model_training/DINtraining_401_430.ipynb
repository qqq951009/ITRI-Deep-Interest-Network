{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from deepctr.models import DeepFM,DIN\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat, DenseFeat, get_feature_names\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_variable(v,filename):\n",
    "    f=open(filename,'wb')\n",
    "    pickle.dump(v,f)\n",
    "    f.close()\n",
    "    return filename\n",
    "  \n",
    "def load_variable(filename):\n",
    "    f=open(filename,'rb')\n",
    "    r=pickle.load(f)\n",
    "    f.close()\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-bigquery\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/1e/bf82824278ed6f430decb33c36f2ed68841307580614ac69d8ee9fecdf51/google_cloud_bigquery-2.23.2-py2.py3-none-any.whl (196kB)\n",
      "\u001b[K    100% |████████████████████████████████| 204kB 2.8MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (3.17.3)\n",
      "Collecting grpcio<2.0dev,>=1.38.1 (from google-cloud-bigquery)\n",
      "  Using cached https://files.pythonhosted.org/packages/66/61/680d8a5a146e33ac463e0fa800ee37c6becbeee4aa02522fa6632dca8831/grpcio-1.39.0-cp37-cp37m-manylinux2010_x86_64.whl\n",
      "Collecting google-resumable-media<3.0dev,>=0.6.0 (from google-cloud-bigquery)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/2b/e342559b2e7c125c06b33f6a8028afaf35378202b6764b41894ab5fb48f5/google_resumable_media-1.3.3-py2.py3-none-any.whl (75kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 4.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery) (2.21.0)\n",
      "Collecting packaging>=14.3 (from google-cloud-bigquery)\n",
      "  Using cached https://files.pythonhosted.org/packages/3c/77/e2362b676dc5008d81be423070dd9577fa03be5da2ba1105811900fda546/packaging-21.0-py3-none-any.whl\n",
      "Collecting proto-plus>=1.10.0 (from google-cloud-bigquery)\n",
      "  Using cached https://files.pythonhosted.org/packages/8c/72/6f3f4cdc5bb0294f8d7f3f8aacb617b4c3cb17554ed78f7e28009162c795/proto_plus-1.19.0-py3-none-any.whl\n",
      "Collecting google-cloud-core<3.0.0dev,>=1.4.1 (from google-cloud-bigquery)\n",
      "  Downloading https://files.pythonhosted.org/packages/f0/55/e8253cfd0b811cd0dc934a980c3842bfb8064944e363c1e6245f0c8a46c6/google_cloud_core-1.7.2-py2.py3-none-any.whl\n",
      "Collecting google-api-core[grpc]<3.0.0dev,>=1.29.0 (from google-cloud-bigquery)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/66/2770f0ab68394711e8a299d97b268fa31fc11ba3ac22885096db14a74fcc/google_api_core-1.31.1-py2.py3-none-any.whl (93kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 4.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.9 in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.12.0->google-cloud-bigquery) (1.12.0)\n",
      "Collecting google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\" (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery)\n",
      "  Using cached https://files.pythonhosted.org/packages/8f/9e/a45476e0fda1ed17e8cc926f4c835e7ae1e2b6460dd00c35a4414ff5ecfa/google_crc32c-1.1.2-cp37-cp37m-manylinux2010_x86_64.whl\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (1.24.1)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (2019.3.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (3.0.4)\n",
      "Collecting pyparsing>=2.0.2 (from packaging>=14.3->google-cloud-bigquery)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: google-auth<2.0dev,>=1.24.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<3.0.0dev,>=1.4.1->google-cloud-bigquery) (1.34.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (57.4.0)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0 (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery)\n",
      "  Using cached https://files.pythonhosted.org/packages/55/08/796a6bc0b550e2b7116041c953d3d5100016abea106131d71e5651826e7b/googleapis_common_protos-1.53.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (2021.1)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.12.2)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.24.0->google-cloud-core<3.0.0dev,>=1.4.1->google-cloud-bigquery) (4.7.2)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.24.0->google-cloud-core<3.0.0dev,>=1.4.1->google-cloud-bigquery) (4.2.2)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2.0dev,>=1.24.0->google-cloud-core<3.0.0dev,>=1.4.1->google-cloud-bigquery) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"->google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (2.19)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2.0dev,>=1.24.0->google-cloud-core<3.0.0dev,>=1.4.1->google-cloud-bigquery) (0.4.8)\n",
      "\u001b[31mtensorflow-gpu 2.5.0 has requirement grpcio~=1.34.0, but you'll have grpcio 1.39.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorflow-gpu 2.5.0 has requirement h5py~=3.1.0, but you'll have h5py 2.10.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorflow-gpu 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.21.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorflow-gpu 2.5.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mgoogle-api-core 1.31.1 has requirement six>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: grpcio, google-crc32c, google-resumable-media, pyparsing, packaging, proto-plus, googleapis-common-protos, google-api-core, google-cloud-core, google-cloud-bigquery\n",
      "  Found existing installation: grpcio 1.34.1\n",
      "    Uninstalling grpcio-1.34.1:\n",
      "      Successfully uninstalled grpcio-1.34.1\n",
      "Successfully installed google-api-core-1.31.1 google-cloud-bigquery-2.23.2 google-cloud-core-1.7.2 google-crc32c-1.1.2 google-resumable-media-1.3.3 googleapis-common-protos-1.53.0 grpcio-1.39.0 packaging-21.0 proto-plus-1.19.0 pyparsing-2.4.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3296: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4162617, 4439353, 3848133, 2049848)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4月的\n",
    "df1 = pd.read_csv('/home/jovyan/dataset/april_csvfile/0401_0410.csv')\n",
    "df2 = pd.read_csv('/home/jovyan/dataset/april_csvfile/0411_0420.csv')\n",
    "df3 = pd.read_csv('/home/jovyan/dataset/april_csvfile/0421_0430.csv')\n",
    "#5月的\n",
    "df4 = pd.read_csv('/home/jovyan/dataset/0501_0505.csv')\n",
    "len(df1),len(df2),len(df3),len(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whichdate(dates):\n",
    "    if dates == '2021-05-01':\n",
    "        return 51\n",
    "    elif dates == '2021-05-02':\n",
    "        return 52\n",
    "    elif dates == '2021-05-03':\n",
    "        return 53\n",
    "    elif dates == '2021-05-04':\n",
    "        return 54\n",
    "    else:\n",
    "        return 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df4.sort_values('api_logtime')\n",
    "df4_month = df4.api_logtime.apply(lambda x:whichdate(x.split(' ')[0]))\n",
    "df4['month'] = df4_month#df4是5月的df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm4 = pd.concat([df1,df2,df3], ignore_index=True)#dfm4 --> 4月的df\n",
    "month1 = len(dfm4)*[4]\n",
    "dfm4['month'] = month1\n",
    "df = pd.concat([dfm4,df4],ignore_index=True)# 合併4月和5月的df\n",
    "df['uid'].fillna(\"unknow\", inplace = True)#uid是空的填上unknow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbe = LabelEncoder()\n",
    "for i in ['gid']:\n",
    "    df['gid_encode'] = lbe.fit_transform(df[i])\n",
    "    df['gid_encode'] = df['gid_encode'].apply(lambda x:x+1)\n",
    "#產生gid和對應id的dictionary\n",
    "temp = df[['gid','gid_encode']].sort_values('gid_encode')\n",
    "gid_encode_unique = temp.gid_encode.unique()\n",
    "gid_unique = temp.gid.unique()\n",
    "gid_dict = {}\n",
    "for i in zip(gid_encode_unique,gid_unique):\n",
    "    gid_dict[i[1]] = i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uid = df[df.uid !='unknow']\n",
    "df_ven_guid = df[df.uid == 'unknow']\n",
    "df_uid = df_uid.drop(columns = ['ven_guid','ven_session'])\n",
    "df_ven_guid = df_ven_guid.drop(columns = ['uid'])\n",
    "df_uid = df_uid.sort_values('api_logtime')\n",
    "df_ven_guid = df_ven_guid.sort_values('api_logtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uid_4 = df_uid[df_uid.month == 4]\n",
    "df_uid_5 = df_uid[df_uid.month != 4]\n",
    "df_ven_guid_4 = df_ven_guid[df_ven_guid.month == 4]\n",
    "df_ven_guid_5 = df_ven_guid[df_ven_guid.month != 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3792823, 613520, 8657280, 1436328)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_uid_4),len(df_uid_5),len(df_ven_guid_4),len(df_ven_guid_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uid_labelencode(df):\n",
    "    for i in ['uid']:\n",
    "        df[i] = lbe.fit_transform(df[i])\n",
    "        df[i] = df[i].apply(lambda x:x+1)\n",
    "def ven_guid_labelencode(df):\n",
    "    for i in ['ven_guid']:\n",
    "        df[i] = lbe.fit_transform(df[i])\n",
    "        df[i] = df[i].apply(lambda x:x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "uid_labelencode(df_uid_4)\n",
    "uid_labelencode(df_uid_5)\n",
    "ven_guid_labelencode(df_ven_guid_4)\n",
    "ven_guid_labelencode(df_ven_guid_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#刪掉點擊商品小於三個的uid或ven_guid\n",
    "df_uid_4 = df_uid_4[df_uid_4['uid'].map(df_uid_4.uid.value_counts())>=3]\n",
    "df_uid_5 = df_uid_5[df_uid_5['uid'].map(df_uid_5.uid.value_counts())>=3]\n",
    "df_ven_guid_4 = df_ven_guid_4[df_ven_guid_4['ven_guid'].map(df_ven_guid_4.ven_guid.value_counts())>=3]\n",
    "df_ven_guid_5 = df_ven_guid_5[df_ven_guid_5['ven_guid'].map(df_ven_guid_5.ven_guid.value_counts())>=3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#計算每個商品在所有商品中(4/1-5/5)的出現頻率\n",
    "under = 0\n",
    "freq_list = []\n",
    "freq = df.gid_encode.value_counts()\n",
    "for i in freq.values:\n",
    "    under += np.log10(i+1)\n",
    "for i in freq.values:\n",
    "    freq_list.append(np.log10(i+1)/under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def create_set_byidname(df,idname):\n",
    "    t_set = []\n",
    "    neg_list = random.choices(freq.index,weights = freq_list, k=1000000)\n",
    "    for userid, hist in df.groupby(idname):\n",
    "        pos_list = hist['gid_encode'].tolist()\n",
    "        if len(pos_list) > 101:#如果pos_list(clickstream)長度大於100，就取後面100筆\n",
    "            pos_list = pos_list[-101:]\n",
    "        def gen_neg():\n",
    "            count = 0\n",
    "            neg = pos_list[0]\n",
    "            while neg in pos_list:\n",
    "                if count == 100:#如果抽100次的負面樣本都抽到在pos_list的,就把count歸0\n",
    "                    count = 0\n",
    "                neg = random.choices(neg_list, k=1)[0]\n",
    "                count += 1\n",
    "            return neg\n",
    "        neg_list2 = [gen_neg() for i in range(len(pos_list))]\n",
    "        for i in range(1, len(pos_list)):\n",
    "            hist = pos_list[:i] \n",
    "            t_set.append((userid, hist, pos_list[i], 1))\n",
    "            t_set.append((userid, hist, neg_list2[i], 0))\n",
    "    return t_set\n",
    "df_train_uid = create_set_byidname(df_uid_4,'uid')\n",
    "df_train_venguid = create_set_byidname(df_ven_guid_4,'ven_guid')\n",
    "train_set = df_train_venguid + df_train_uid\n",
    "np.random.shuffle(train_set)\n",
    "len(df_train_venguid),len(df_train_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_array(data):\n",
    "    g_id = []\n",
    "    hist_g_id = []\n",
    "    click = []\n",
    "    seq_length = []\n",
    "\n",
    "    for index, value in enumerate(data):\n",
    "        hist_g_id.append(value[1])\n",
    "        seq_length.append(len(value[1]))\n",
    "        g_id.append(value[2])\n",
    "        click.append(value[3])  \n",
    "    for i in range(len(hist_g_id)):\n",
    "        if len(hist_g_id[i]) != 100:\n",
    "            hist_g_id[i] += (100-len(hist_g_id[i]))*[0]\n",
    "\n",
    "    g_id = np.array(g_id)\n",
    "    hist_g_id = np.array(hist_g_id)\n",
    "    click = np.array(click)\n",
    "    seq_length = np.array(seq_length)\n",
    "    return g_id, hist_g_id, click, seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_id_train, hist_g_id_train, click_train, seq_length_train = create_array(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1824365, 1278717,  936515, ...,  613386, 1055567,  378817]),\n",
       " array([[ 149969,  972274,  972274, ...,       0,       0,       0],\n",
       "        [ 250176,  836563, 1444467, ...,       0,       0,       0],\n",
       "        [1216868, 1216868, 1460994, ...,       0,       0,       0],\n",
       "        ...,\n",
       "        [1359690, 1359690,  496823, ...,       0,       0,       0],\n",
       "        [1224981, 1224981, 1224981, ...,       0,       0,       0],\n",
       "        [1053905, 1773460, 1053905, ...,       0,       0,       0]]),\n",
       " array([1, 0, 0, ..., 1, 1, 0]),\n",
       " array([20,  6,  6, ..., 53,  5,  3]))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_id_train, hist_g_id_train, click_train, seq_length_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_feature_list = [\"g_id\"]\n",
    "feature_columns = [SparseFeat('g_id', df.gid_encode.max()+1, embedding_dim=8)]\n",
    "feature_columns += [VarLenSparseFeat(SparseFeat('hist_g_id',vocabulary_size=df.gid_encode.max()+1, embedding_dim=8, embedding_name='g_id'), maxlen = 100, length_name='seq_length')]\n",
    "x = {'g_id': g_id_train, \n",
    "    'hist_g_id': hist_g_id_train, \n",
    "    'seq_length': seq_length_train}\n",
    "y = click_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g_id\n",
      "(14259236,)\n",
      "hist_g_id\n",
      "(14259236, 100)\n",
      "seq_length\n",
      "(14259236,)\n"
     ]
    }
   ],
   "source": [
    "for i, v in x.items():\n",
    "    print(i)\n",
    "    print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "111401/111401 [==============================] - 26833s 241ms/step - loss: 0.6617 - accuracy: 0.5709 - auc: 0.6167\n",
      "\n",
      "Epoch 00001: saving model to /home/jovyan/dataset/rakuten_0811_cp1.ckpt\n",
      "Epoch 2/10\n",
      "111401/111401 [==============================] - 27634s 248ms/step - loss: 0.5416 - accuracy: 0.7302 - auc: 0.7958\n",
      "\n",
      "Epoch 00002: saving model to /home/jovyan/dataset/rakuten_0811_cp1.ckpt\n",
      "Epoch 3/10\n",
      "111401/111401 [==============================] - 27802s 250ms/step - loss: 0.4727 - accuracy: 0.7887 - auc: 0.8534\n",
      "\n",
      "Epoch 00003: saving model to /home/jovyan/dataset/rakuten_0811_cp1.ckpt\n",
      "Epoch 4/10\n",
      "111401/111401 [==============================] - 27839s 250ms/step - loss: 0.4215 - accuracy: 0.8210 - auc: 0.8867\n",
      "\n",
      "Epoch 00004: saving model to /home/jovyan/dataset/rakuten_0811_cp1.ckpt\n",
      "Epoch 5/10\n",
      "111401/111401 [==============================] - 28679s 257ms/step - loss: 0.3788 - accuracy: 0.8430 - auc: 0.9104\n",
      "\n",
      "Epoch 00005: saving model to /home/jovyan/dataset/rakuten_0811_cp1.ckpt\n",
      "Epoch 6/10\n",
      "111401/111401 [==============================] - 28603s 257ms/step - loss: 0.3419 - accuracy: 0.8602 - auc: 0.9280\n",
      "\n",
      "Epoch 00006: saving model to /home/jovyan/dataset/rakuten_0811_cp1.ckpt\n",
      "Epoch 7/10\n",
      "111401/111401 [==============================] - 24847s 223ms/step - loss: 0.3089 - accuracy: 0.8748 - auc: 0.9419\n",
      "\n",
      "Epoch 00007: saving model to /home/jovyan/dataset/rakuten_0811_cp1.ckpt\n",
      "Epoch 8/10\n",
      "111401/111401 [==============================] - 29728s 267ms/step - loss: 0.2832 - accuracy: 0.8857 - auc: 0.9515\n",
      "\n",
      "Epoch 00008: saving model to /home/jovyan/dataset/rakuten_0811_cp1.ckpt\n",
      "Epoch 9/10\n",
      "111401/111401 [==============================] - 33467s 300ms/step - loss: 0.2594 - accuracy: 0.8959 - auc: 0.9595\n",
      "\n",
      "Epoch 00009: saving model to /home/jovyan/dataset/rakuten_0811_cp1.ckpt\n",
      "Epoch 10/10\n",
      "111401/111401 [==============================] - 31866s 286ms/step - loss: 0.2403 - accuracy: 0.9041 - auc: 0.9654\n",
      "\n",
      "Epoch 00010: saving model to /home/jovyan/dataset/rakuten_0811_cp1.ckpt\n"
     ]
    }
   ],
   "source": [
    "model = DIN(feature_columns, behavior_feature_list)\n",
    "model.compile('SGD', 'binary_crossentropy',metrics=['accuracy','AUC'])\n",
    "checkpoint_path = \"/home/jovyan/dataset/checkpoint_folder/model_401_430/rakuten_0401_0430.ckpt\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True,verbose=1) \n",
    "history = model.fit(x, y, batch_size=128, verbose=1, epochs=10, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "111401/111401 [==============================] - 28619s 257ms/step - loss: 0.3826 - accuracy: 0.8253 - auc: 0.9074\n",
      "\n",
      "Epoch 00001: saving model to /home/jovyan/dataset/15epoch/401_430.ckpt\n",
      "Epoch 2/5\n",
      "111401/111401 [==============================] - 26982s 242ms/step - loss: 0.3202 - accuracy: 0.8616 - auc: 0.9370\n",
      "\n",
      "Epoch 00002: saving model to /home/jovyan/dataset/15epoch/401_430.ckpt\n",
      "Epoch 3/5\n",
      "111401/111401 [==============================] - 26730s 240ms/step - loss: 0.2820 - accuracy: 0.8819 - auc: 0.9519\n",
      "\n",
      "Epoch 00003: saving model to /home/jovyan/dataset/15epoch/401_430.ckpt\n",
      "Epoch 4/5\n",
      "111401/111401 [==============================] - 27007s 242ms/step - loss: 0.2547 - accuracy: 0.8953 - auc: 0.9612\n",
      "\n",
      "Epoch 00004: saving model to /home/jovyan/dataset/15epoch/401_430.ckpt\n",
      "Epoch 5/5\n",
      "111401/111401 [==============================] - 27036s 243ms/step - loss: 0.2342 - accuracy: 0.9051 - auc: 0.9674\n",
      "\n",
      "Epoch 00005: saving model to /home/jovyan/dataset/15epoch/401_430.ckpt\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "model = keras.models.load_model(\"/home/jovyan/dataset/rakuten_model_401_430\")\n",
    "checkpoint_path = \"/home/jovyan/dataset/15epoch/401_430.ckpt\"\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True,verbose=1)\n",
    "history = model.fit(x, y, batch_size=128, verbose=1, epochs=5, callbacks=[cp_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
