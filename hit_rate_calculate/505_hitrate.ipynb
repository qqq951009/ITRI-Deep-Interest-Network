{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from deepctr.models import DeepFM,DIN\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat, DenseFeat, get_feature_names\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pickle\n",
    "import keras\n",
    "def save_variable(v,filename):\n",
    "    f=open(filename,'wb')\n",
    "    pickle.dump(v,f)\n",
    "    f.close()\n",
    "    return filename\n",
    "def load_variable(filename):\n",
    "    f=open(filename,'rb')\n",
    "    r=pickle.load(f)\n",
    "    f.close()\n",
    "    return r\n",
    "gid_dict = load_variable('/home/jovyan/dataset/gid_dict')\n",
    "gid_list = load_variable('/home/jovyan/dataset/gid_list')#len = 1739595\n",
    "df_val = pd.read_csv('/home/jovyan/dataset/validate_folder/505val.csv')\n",
    "df_candidate = load_variable('/home/jovyan/dataset/original_candidate_itemset/0505_candidate_itemset')#len = 1422941"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py:5507: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889285\n"
     ]
    }
   ],
   "source": [
    "def clean_candidate(df):#舊方法刪完後的結果(不存在就刪掉)\n",
    "    df = df[df['gid'].isin(gid_list)]\n",
    "    df.candidate_items = df.candidate_items.apply(lambda x:x.split(','))\n",
    "    df = df.explode('candidate_items', ignore_index=True)\n",
    "    df = df[df['candidate_items'].isin(gid_list)]#推薦清單 candidate_item清掉不在4/1-4/30的商品\n",
    "    df.gid = df.gid.apply(lambda x:gid_dict[x])\n",
    "    df.candidate_items = df.candidate_items.apply(lambda x:gid_dict[x])\n",
    "    df = df.groupby(['gid']).agg({'candidate_items': lambda x: x.tolist()})\n",
    "    df = df[df.candidate_items.apply(lambda x:len(x)) >15]\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "df_candidate = clean_candidate(df_candidate)#889285\n",
    "df_candidate['original_score'] = df_candidate.candidate_items.apply(lambda x:list(np.arange(1,0,-1/len(x))))\n",
    "print(len(df_candidate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "df5_all = load_variable('/home/jovyan/dataset/df5_all')\n",
    "df_uid = df5_all[df5_all['uid'] != 'unknow']\n",
    "df_session = df5_all[df5_all['uid'] == 'unknow']\n",
    "lbe = LabelEncoder()\n",
    "for i in ['uid']:\n",
    "    df_uid['uid_encode'] = lbe.fit_transform(df_uid[i])\n",
    "    df_uid['uid_encode'] = df_uid['uid_encode'].apply(lambda x:x+1)\n",
    "for i in ['ven_session']:\n",
    "    df_session['session_encode'] = lbe.fit_transform(df_session[i])\n",
    "    df_session['session_encode'] = df_session['session_encode'].apply(lambda x:x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid_encode_unique = df_uid[['uid','uid_encode']].sort_values('uid_encode').uid_encode.unique()\n",
    "uid_unique = df_uid[['uid','uid_encode']].sort_values('uid_encode').uid.unique()\n",
    "uid_dict = {}\n",
    "for i in zip(uid_encode_unique,uid_unique):\n",
    "    uid_dict[i[1]] = i[0]\n",
    "session_encode_unique = df_session[['ven_session','session_encode']].sort_values('session_encode').session_encode.unique()\n",
    "session_unique = df_session[['ven_session','session_encode']].sort_values('session_encode').ven_session.unique()\n",
    "session_dict = {}\n",
    "for i in zip(session_encode_unique,session_unique):\n",
    "    session_dict[i[1]] = i[0]\n",
    "session_dict[0] = 0\n",
    "uid_dict[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uid5 = df_uid[df_uid['month'] == 55]\n",
    "df_session5 = df_session[df_session['month'] == 55]\n",
    "df_uid5.uid = df_uid5.uid.apply(lambda x:uid_dict[x])\n",
    "df_session5.ven_session = df_session5.ven_session.apply(lambda x:session_dict[x])\n",
    "df_session5 = df_session5.drop(columns = ['uid','ven_guid'])\n",
    "df_uid5 = df_uid5.drop(columns=['ven_guid','ven_session'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85158, 59257)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_session = df_val[df_val.uid.isna()]\n",
    "df_val_uid = df_val[~df_val.uid.isna()]\n",
    "len(df_val_session),len(df_val_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_session.ven_session = df_val_session.ven_session.apply(lambda x:session_dict[x])\n",
    "df_val_uid.uid = df_val_uid.uid.apply(lambda x:uid_dict[x])\n",
    "df_val_session.gid = df_val_session.gid.apply(lambda x:gid_dict[x])\n",
    "df_val_uid.gid = df_val_uid.gid.apply(lambda x:gid_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "def process(df_val,df,idname):\n",
    "    hist = []\n",
    "    id_list = df_val[idname].unique().tolist()\n",
    "    for i in id_list:\n",
    "        tempdf = df[df[idname] == i]\n",
    "        tempdf_copy = df[df[idname] == i]\n",
    "        gid_list = df_val[df_val[idname] == i].gid.tolist()\n",
    "        for j in gid_list:\n",
    "            index = tempdf[tempdf.gid_encode == j].index[0]\n",
    "            tempdf = tempdf.drop(index)\n",
    "            hist.append(tempdf_copy.loc[:index].gid_encode.tolist())\n",
    "    df_val['hist'] = hist\n",
    "    return df_val\n",
    "df_val_session = process(df_val_session,df_session5,'ven_session')\n",
    "df_val_uid = process(df_val_uid,df_uid5,'uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54853 41151\n"
     ]
    }
   ],
   "source": [
    "result_session = pd.merge(df_candidate,df_val_session,how = 'inner',on = ['gid'])\n",
    "result_uid = pd.merge(df_candidate,df_val_uid,how = 'inner',on = ['gid'])\n",
    "result_session = result_session[['ven_session','gid','hist','next_gid','candidate_items','original_score']]\n",
    "result_uid = result_uid[['uid','gid','hist','next_gid','candidate_items','original_score']]\n",
    "result_session = result_session.sort_values('ven_session')\n",
    "result_uid = result_uid.sort_values('uid')\n",
    "result_session.next_gid = result_session.next_gid.apply(lambda x:gid_dict[x])\n",
    "result_uid.next_gid = result_uid.next_gid.apply(lambda x:gid_dict[x])\n",
    "result_session['flag'] = list(range(1,len(result_session)+1))\n",
    "result_uid['flag'] = list(range(1,len(result_uid)+1))\n",
    "print(len(result_session),len(result_uid)) #\n",
    "result_session_explode = result_session.explode(['candidate_items','original_score'], ignore_index=True)\n",
    "result_uid_explode = result_uid.explode(['candidate_items','original_score'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pred_set(result):\n",
    "    pred_list = []\n",
    "    hist = 0\n",
    "    for i in zip(result['hist'].tolist(),result['candidate_items'].tolist()):\n",
    "        if len(i[0])>100:\n",
    "            pred_list.append((i[0][-100:],i[1],100))\n",
    "        else:\n",
    "            pred_list.append((i[0],i[1],len(i[0])))\n",
    "    return pred_list\n",
    "pred_set_session = create_pred_set(result_session_explode)\n",
    "pred_set_uid = create_pred_set(result_uid_explode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predict_array(pred_set):\n",
    "    hist_g_id = []\n",
    "    g_id = []\n",
    "    seq_length = []\n",
    "    for index, value in enumerate(pred_set):\n",
    "        hist_g_id.append(value[0]) \n",
    "        g_id.append(value[1])\n",
    "        seq_length.append(value[2])\n",
    "    for i in range(len(hist_g_id)):\n",
    "        if len(hist_g_id[i]) < 100:\n",
    "            hist_g_id[i] += (100-len(hist_g_id[i]))*[0]\n",
    "    g_id = np.array(g_id)\n",
    "    hist_g_id = np.array(hist_g_id)\n",
    "    seq_length = np.array(seq_length)\n",
    "    return hist_g_id, g_id, seq_length\n",
    "\n",
    "hist_gid_session, gid_session, seq_length_session = create_predict_array(pred_set_session)\n",
    "hist_gid_uid, gid_uid, seq_length_uid = create_predict_array(pred_set_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_uid_pred = {'g_id': gid_uid, \n",
    "        'hist_g_id': hist_gid_uid, \n",
    "        'seq_length': seq_length_uid}\n",
    "x_session_pred = {'g_id': gid_session, \n",
    "        'hist_g_id': hist_gid_session, \n",
    "        'seq_length': seq_length_session}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4/1-5/4 predict 5/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_feature_list = [\"g_id\"]\n",
    "feature_columns = [SparseFeat('g_id', 1870599, embedding_dim=8)]\n",
    "feature_columns += [VarLenSparseFeat(SparseFeat('hist_g_id',vocabulary_size=1870599, embedding_dim=8, embedding_name='g_id'), maxlen = 100, length_name='seq_length')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fcb243ba320>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DIN(feature_columns, behavior_feature_list)\n",
    "model.compile('SGD', 'binary_crossentropy',metrics=['accuracy','AUC'])\n",
    "latest = tf.train.latest_checkpoint(\"/home/jovyan/dataset/checkpoint_folder/model_401_504\")\n",
    "model.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_uid = model.predict(x_uid_pred)\n",
    "y_session = model.predict(x_session_pred)\n",
    "y_uid = y_uid.reshape(-1).tolist()\n",
    "y_session = y_session.reshape(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_uid_explode['score'] = y_uid\n",
    "result_session_explode['score'] = y_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_new_score(result):\n",
    "    new_score = []\n",
    "    for i in result.iterrows():\n",
    "        new_score.append(0.2*(i[1]['score'])+0.8*(i[1]['original_score']))\n",
    "    result['new_score'] = new_score\n",
    "    result = result.sort_values(['flag','new_score'],ascending = False)\n",
    "    df_output = result.groupby(['flag','gid','next_gid'],as_index=False).agg({'candidate_items': lambda x: x.tolist()})\n",
    "    return df_output\n",
    "df1 = cal_new_score(result_uid_explode)\n",
    "df2 = cal_new_score(result_session_explode)\n",
    "resultdf = pd.concat([df1,df2],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23394858547560518 0.3234031915336861 0.3703283196533478\n"
     ]
    }
   ],
   "source": [
    "top5 = hitrate(resultdf,5)\n",
    "top10 = hitrate(resultdf,10)\n",
    "top15 = hitrate(resultdf,15)\n",
    "print(top5,top10,top15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22619890837881754 0.32001791592017 0.3692346152243656\n"
     ]
    }
   ],
   "source": [
    "def hitrate(result,k):\n",
    "    count = 0\n",
    "    for i in result.iterrows():\n",
    "        if i[1]['next_gid'] in i[1]['candidate_items'][:k]:\n",
    "            count+=1\n",
    "    return count/len(result)\n",
    "top5 = hitrate(resultdf,5)\n",
    "top10 = hitrate(resultdf,10)\n",
    "top15 = hitrate(resultdf,15)\n",
    "print(top5,top10,top15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using epoch = 15 model predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc95c60e0f0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behavior_feature_list = [\"g_id\"]\n",
    "feature_columns = [SparseFeat('g_id', 1870599, embedding_dim=8)]\n",
    "feature_columns += [VarLenSparseFeat(SparseFeat('hist_g_id',vocabulary_size=1870599, embedding_dim=8, embedding_name='g_id'), maxlen = 100, length_name='seq_length')]\n",
    "model15 = DIN(feature_columns, behavior_feature_list)\n",
    "model15.compile('SGD', 'binary_crossentropy',metrics=['accuracy','AUC'])\n",
    "latest = tf.train.latest_checkpoint(\"/home/jovyan/dataset/15epoch/model401_504\")\n",
    "model15.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_uid15 = model15.predict(x_uid_pred)\n",
    "y_session15 = model15.predict(x_session_pred)\n",
    "y_uid15 = y_uid15.reshape(-1).tolist()\n",
    "y_session15 = y_session15.reshape(-1).tolist()\n",
    "result_uid_explode['score'] = y_uid15\n",
    "result_session_explode['score'] = y_session15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14770217907587183 0.23400066663889005 0.3038519228365485\n"
     ]
    }
   ],
   "source": [
    "epoch15_df1 = result_uid_explode.sort_values(['flag','score'],ascending = False)\n",
    "epoch15_df2 = result_session_explode.sort_values(['flag','score'],ascending = False)\n",
    "epoch15_df1 = epoch15_df1.groupby(['flag','gid','next_gid'],as_index=False).agg({'candidate_items': lambda x: x.tolist()})\n",
    "epoch15_df2 = epoch15_df2.groupby(['flag','gid','next_gid'],as_index=False).agg({'candidate_items': lambda x: x.tolist()})\n",
    "epoch15_df = pd.concat([epoch15_df1,epoch15_df2],ignore_index=True)\n",
    "top5 = hitrate(epoch15_df,5)\n",
    "top10 = hitrate(epoch15_df,10)\n",
    "top15 = hitrate(epoch15_df,15)\n",
    "print(top5,top10,top15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### epoch 20 model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-678a4104b12b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel20\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0my_uid20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel20\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_uid_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0my_session20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel20\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_session_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my_uid20\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_uid20\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1725\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1954\u001b[0m                          \"Tensor.\" % (self._func_graph.name, i, str(arg)))\n\u001b[1;32m   1955\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_inputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1956\u001b[0;31m     \u001b[0mpossible_gradient_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradients_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPossibleTapeGradientTypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1957\u001b[0m     if (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model20 = DIN(feature_columns, behavior_feature_list)\n",
    "model20.compile('SGD', 'binary_crossentropy',metrics=['accuracy','AUC'])\n",
    "latest = tf.train.latest_checkpoint(\"/home/jovyan/dataset/20epoch/model401_504\")\n",
    "model20.load_weights(latest)\n",
    "\n",
    "y_uid20 = model20.predict(x_uid_pred)\n",
    "y_session20 = model20.predict(x_session_pred)\n",
    "y_uid20 = y_uid20.reshape(-1).tolist()\n",
    "y_session20 = y_session20.reshape(-1).tolist()\n",
    "result_uid_explode['score'] = y_uid20\n",
    "result_session_explode['score'] = y_session20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14646264738969209 0.23405274780217492 0.3043727344693971\n"
     ]
    }
   ],
   "source": [
    "epoch20_df1 = result_uid_explode.sort_values(['flag','score'],ascending = False)\n",
    "epoch20_df2 = result_session_explode.sort_values(['flag','score'],ascending = False)\n",
    "epoch20_df1 = epoch20_df1.groupby(['flag','gid','next_gid'],as_index=False).agg({'candidate_items': lambda x: x.tolist()})\n",
    "epoch20_df2 = epoch20_df2.groupby(['flag','gid','next_gid'],as_index=False).agg({'candidate_items': lambda x: x.tolist()})\n",
    "epoch20_df = pd.concat([epoch20_df1,epoch20_df2],ignore_index=True)\n",
    "\n",
    "top5 = hitrate(epoch20_df,5)\n",
    "top10 = hitrate(epoch20_df,10)\n",
    "top15 = hitrate(epoch20_df,15)\n",
    "print(top5,top10,top15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val1 = df_val_session[df_val_session.gid.isin(df_candidate.gid.tolist())]\n",
    "df_val2 = df_val_uid[df_val_uid.gid.isin(df_candidate.gid.tolist())]\n",
    "df_val_baseline =  pd.concat([df_val1,df_val2],ignore_index=True)\n",
    "df_val_baseline.next_gid = df_val_baseline.next_gid.apply(lambda x:gid_dict[x])\n",
    "df_val_baseline = pd.merge(df_candidate,df_val_baseline,how = 'inner',on = ['gid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitrate_baseline(df,k):\n",
    "    count = 0\n",
    "    for i in df.iterrows():\n",
    "        if i[1]['next_gid'] in i[1]['candidate_items'][:k]:\n",
    "            count += 1 \n",
    "    return count/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2346673055289363 0.3220386650556227 0.36870338735886005\n"
     ]
    }
   ],
   "source": [
    "top5_b = hitrate_baseline(df_val_baseline,5)\n",
    "top10_b = hitrate_baseline(df_val_baseline,10) \n",
    "top15_b = hitrate_baseline(df_val_baseline,15) \n",
    "print(top5_b,top10_b,top15_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
